{"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71549,"databundleVersionId":8561470,"sourceType":"competition"}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.14"},"papermill":{"default_parameters":{},"duration":5999.360483,"end_time":"2024-10-28T15:58:04.506965","environment_variables":{},"exception":true,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-10-28T14:18:05.146482","version":"2.6.0"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"#in house cnn\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nimport os\nimport time\nimport numpy as np\nimport glob\nimport json\nimport collections\nimport torch\nimport torch.nn as nn\n\nimport pydicom as dicom\nimport matplotlib.patches as patches\n\nfrom matplotlib import animation, rc\nimport pandas as pd\nimport pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as transforms\nimport torch\nimport torch.optim.lr_scheduler as lr_scheduler\nimport torch.optim as optim\nfrom copy import deepcopy\nfrom tqdm import tqdm","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":7.120493,"end_time":"2024-10-28T14:18:14.961023","exception":false,"start_time":"2024-10-28T14:18:07.840530","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:53.011654Z","iopub.execute_input":"2024-10-29T02:07:53.012045Z","iopub.status.idle":"2024-10-29T02:07:53.019882Z","shell.execute_reply.started":"2024-10-29T02:07:53.012007Z","shell.execute_reply":"2024-10-29T02:07:53.018982Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"root = '/kaggle/input/rsna-2024-lumbar-spine-degenerative-classification/'\ntrain  = pd.read_csv(root + 'train.csv')\nlabel = pd.read_csv(root + 'train_label_coordinates.csv')\ntrain_desc  = pd.read_csv(root + 'train_series_descriptions.csv')\ntest_desc   = pd.read_csv(root + 'test_series_descriptions.csv')\nsub         = pd.read_csv(root + 'sample_submission.csv')\nconditions = {}","metadata":{"papermill":{"duration":0.162167,"end_time":"2024-10-28T14:18:15.130138","exception":false,"start_time":"2024-10-28T14:18:14.967971","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:53.021503Z","iopub.execute_input":"2024-10-29T02:07:53.021825Z","iopub.status.idle":"2024-10-29T02:07:53.128928Z","shell.execute_reply.started":"2024-10-29T02:07:53.021793Z","shell.execute_reply":"2024-10-29T02:07:53.128143Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def generate_image_paths(df, data_dir):\n    image_paths = []\n    for study_id, series_id in zip(df['study_id'], df['series_id']):\n        study_dir = os.path.join(data_dir, str(study_id))\n        series_dir = os.path.join(study_dir, str(series_id))\n        images = os.listdir(series_dir)\n        image_paths.extend([os.path.join(series_dir, img) for img in images])\n    return image_paths","metadata":{"papermill":{"duration":0.015107,"end_time":"2024-10-28T14:18:15.151891","exception":false,"start_time":"2024-10-28T14:18:15.136784","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:53.130124Z","iopub.execute_input":"2024-10-29T02:07:53.130511Z","iopub.status.idle":"2024-10-29T02:07:53.137010Z","shell.execute_reply.started":"2024-10-29T02:07:53.130466Z","shell.execute_reply":"2024-10-29T02:07:53.136072Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Function to generate image paths based on directory structure\ndef reshape_row(row):\n    data = {'study_id': [], 'condition': [], 'level': [], 'severity': []}\n    \n    for column, value in row.items():\n        if column not in ['study_id', 'series_id', 'instance_number', 'x', 'y', 'series_description']:\n            parts = column.split('_')\n            condition = ' '.join([word.capitalize() for word in parts[:-2]])\n            level = parts[-2].capitalize() + '/' + parts[-1].capitalize()\n            data['study_id'].append(row['study_id'])\n            data['condition'].append(condition)\n            data['level'].append(level)\n            data['severity'].append(value)\n    \n    return pd.DataFrame(data)","metadata":{"papermill":{"duration":0.016002,"end_time":"2024-10-28T14:18:15.175307","exception":false,"start_time":"2024-10-28T14:18:15.159305","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:53.138236Z","iopub.execute_input":"2024-10-29T02:07:53.138618Z","iopub.status.idle":"2024-10-29T02:07:53.148701Z","shell.execute_reply.started":"2024-10-29T02:07:53.138573Z","shell.execute_reply":"2024-10-29T02:07:53.147881Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def reshape_row_2(row):\n    # Create dictionary to store levels and severities\n    conditions = {}\n\n    # Add level-severity pair directly from the row's values\n    level = row['level']\n    severity = row['severity']\n    \n    # Populate the conditions dictionary with level and severity\n    conditions[level] = severity\n    \n    # Return a dictionary with the original row values and new 'conditions' column\n    return {\n        'study_id': row['study_id'],\n        'series_id': row['series_id'],\n        'instance_number': row['instance_number'],\n        'conditions': conditions\n    }","metadata":{"papermill":{"duration":0.014016,"end_time":"2024-10-28T14:18:15.195702","exception":false,"start_time":"2024-10-28T14:18:15.181686","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:53.151187Z","iopub.execute_input":"2024-10-29T02:07:53.151488Z","iopub.status.idle":"2024-10-29T02:07:53.162931Z","shell.execute_reply.started":"2024-10-29T02:07:53.151456Z","shell.execute_reply":"2024-10-29T02:07:53.162226Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"train_image_paths = generate_image_paths(train_desc, f'{root}/train_images')\ntest_image_paths = generate_image_paths(test_desc, f'{root}/test_images')","metadata":{"papermill":{"duration":42.461795,"end_time":"2024-10-28T14:18:57.663821","exception":false,"start_time":"2024-10-28T14:18:15.202026","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:53.163841Z","iopub.execute_input":"2024-10-29T02:07:53.164090Z","iopub.status.idle":"2024-10-29T02:07:57.424925Z","shell.execute_reply.started":"2024-10-29T02:07:53.164061Z","shell.execute_reply":"2024-10-29T02:07:57.424116Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([reshape_row(row) for _, row in train.iterrows()], ignore_index=True)","metadata":{"papermill":{"duration":1.242827,"end_time":"2024-10-28T14:18:58.913511","exception":false,"start_time":"2024-10-28T14:18:57.670684","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:57.426031Z","iopub.execute_input":"2024-10-29T02:07:57.426310Z","iopub.status.idle":"2024-10-29T02:07:58.864644Z","shell.execute_reply.started":"2024-10-29T02:07:57.426278Z","shell.execute_reply":"2024-10-29T02:07:58.863688Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"#join the labels with the train dataset\ntrain = pd.merge(train, label, on=['study_id', 'condition', 'level'], how='inner')\ntrain = pd.merge(train, train_desc, on=['series_id','study_id'], how='inner')\n","metadata":{"papermill":{"duration":0.092487,"end_time":"2024-10-28T14:18:59.012852","exception":false,"start_time":"2024-10-28T14:18:58.920365","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:58.866008Z","iopub.execute_input":"2024-10-29T02:07:58.866390Z","iopub.status.idle":"2024-10-29T02:07:58.942193Z","shell.execute_reply.started":"2024-10-29T02:07:58.866346Z","shell.execute_reply":"2024-10-29T02:07:58.941295Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"train[\"severity\"] = train[\"severity\"].map({'Normal/Mild':0, 'Severe':2, 'Moderate':1})\ntrain = train.dropna()\ntrain.head()","metadata":{"papermill":{"duration":0.051205,"end_time":"2024-10-28T14:18:59.070654","exception":false,"start_time":"2024-10-28T14:18:59.019449","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:58.943900Z","iopub.execute_input":"2024-10-29T02:07:58.944231Z","iopub.status.idle":"2024-10-29T02:07:58.983896Z","shell.execute_reply.started":"2024-10-29T02:07:58.944184Z","shell.execute_reply":"2024-10-29T02:07:58.982917Z"},"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"   study_id              condition  level  severity  series_id  \\\n0   4003253  Spinal Canal Stenosis  L1/L2       0.0  702807833   \n1   4003253  Spinal Canal Stenosis  L2/L3       0.0  702807833   \n2   4003253  Spinal Canal Stenosis  L3/L4       0.0  702807833   \n3   4003253  Spinal Canal Stenosis  L4/L5       0.0  702807833   \n4   4003253  Spinal Canal Stenosis  L5/S1       0.0  702807833   \n\n   instance_number           x           y series_description  \n0                8  322.831858  227.964602   Sagittal T2/STIR  \n1                8  320.571429  295.714286   Sagittal T2/STIR  \n2                8  323.030303  371.818182   Sagittal T2/STIR  \n3                8  335.292035  427.327434   Sagittal T2/STIR  \n4                8  353.415929  483.964602   Sagittal T2/STIR  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_id</th>\n      <th>condition</th>\n      <th>level</th>\n      <th>severity</th>\n      <th>series_id</th>\n      <th>instance_number</th>\n      <th>x</th>\n      <th>y</th>\n      <th>series_description</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L1/L2</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>322.831858</td>\n      <td>227.964602</td>\n      <td>Sagittal T2/STIR</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L2/L3</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>320.571429</td>\n      <td>295.714286</td>\n      <td>Sagittal T2/STIR</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L3/L4</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>323.030303</td>\n      <td>371.818182</td>\n      <td>Sagittal T2/STIR</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L4/L5</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>335.292035</td>\n      <td>427.327434</td>\n      <td>Sagittal T2/STIR</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L5/S1</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>353.415929</td>\n      <td>483.964602</td>\n      <td>Sagittal T2/STIR</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"train['image_path'] = (\n    f'{root}/train_images/' + \n    train['study_id'].astype(str) + '/' +\n    train['series_id'].astype(str) + '/' +\n    train['instance_number'].astype(str) + '.dcm'\n)\ntrain.head(2)","metadata":{"papermill":{"duration":0.123753,"end_time":"2024-10-28T14:18:59.201486","exception":false,"start_time":"2024-10-28T14:18:59.077733","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:58.985268Z","iopub.execute_input":"2024-10-29T02:07:58.985769Z","iopub.status.idle":"2024-10-29T02:07:59.115484Z","shell.execute_reply.started":"2024-10-29T02:07:58.985721Z","shell.execute_reply":"2024-10-29T02:07:59.114504Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"   study_id              condition  level  severity  series_id  \\\n0   4003253  Spinal Canal Stenosis  L1/L2       0.0  702807833   \n1   4003253  Spinal Canal Stenosis  L2/L3       0.0  702807833   \n\n   instance_number           x           y series_description  \\\n0                8  322.831858  227.964602   Sagittal T2/STIR   \n1                8  320.571429  295.714286   Sagittal T2/STIR   \n\n                                          image_path  \n0  /kaggle/input/rsna-2024-lumbar-spine-degenerat...  \n1  /kaggle/input/rsna-2024-lumbar-spine-degenerat...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_id</th>\n      <th>condition</th>\n      <th>level</th>\n      <th>severity</th>\n      <th>series_id</th>\n      <th>instance_number</th>\n      <th>x</th>\n      <th>y</th>\n      <th>series_description</th>\n      <th>image_path</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L1/L2</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>322.831858</td>\n      <td>227.964602</td>\n      <td>Sagittal T2/STIR</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L2/L3</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>320.571429</td>\n      <td>295.714286</td>\n      <td>Sagittal T2/STIR</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import os\n\n# Define a function to check if a path exists\ndef check_exists(path):\n    return os.path.exists(path)\n\n# Define a function to check if a study ID directory exists\ndef check_study_id(row):\n    study_id = row['study_id']\n    path = f'{root}/train_images/{study_id}'\n    return check_exists(path)\n\n# Define a function to check if a series ID directory exists\ndef check_series_id(row):\n    study_id = row['study_id']\n    series_id = row['series_id']\n    path = f'{root}/train_images/{study_id}/{series_id}'\n    return check_exists(path)\n\n# Define a function to check if an image file exists\ndef check_image_exists(row):\n    image_path = row['image_path']\n    return check_exists(image_path)\n\n# Apply the functions to the train_data dataframe\ntrain['study_id_exists'] = train.apply(check_study_id, axis=1)\ntrain['series_id_exists'] = train.apply(check_series_id, axis=1)\ntrain['image_exists'] = train.apply(check_image_exists, axis=1)\n\n# Filter train_data\ntrain = train[(train['study_id_exists']) & (train['series_id_exists']) & (train['image_exists'])]\ntrain.head(4)","metadata":{"papermill":{"duration":17.236106,"end_time":"2024-10-28T14:19:16.444634","exception":false,"start_time":"2024-10-28T14:18:59.208528","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:07:59.117309Z","iopub.execute_input":"2024-10-29T02:07:59.117753Z","iopub.status.idle":"2024-10-29T02:08:38.894055Z","shell.execute_reply.started":"2024-10-29T02:07:59.117704Z","shell.execute_reply":"2024-10-29T02:08:38.893156Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"   study_id              condition  level  severity  series_id  \\\n0   4003253  Spinal Canal Stenosis  L1/L2       0.0  702807833   \n1   4003253  Spinal Canal Stenosis  L2/L3       0.0  702807833   \n2   4003253  Spinal Canal Stenosis  L3/L4       0.0  702807833   \n3   4003253  Spinal Canal Stenosis  L4/L5       0.0  702807833   \n\n   instance_number           x           y series_description  \\\n0                8  322.831858  227.964602   Sagittal T2/STIR   \n1                8  320.571429  295.714286   Sagittal T2/STIR   \n2                8  323.030303  371.818182   Sagittal T2/STIR   \n3                8  335.292035  427.327434   Sagittal T2/STIR   \n\n                                          image_path  study_id_exists  \\\n0  /kaggle/input/rsna-2024-lumbar-spine-degenerat...             True   \n1  /kaggle/input/rsna-2024-lumbar-spine-degenerat...             True   \n2  /kaggle/input/rsna-2024-lumbar-spine-degenerat...             True   \n3  /kaggle/input/rsna-2024-lumbar-spine-degenerat...             True   \n\n   series_id_exists  image_exists  \n0              True          True  \n1              True          True  \n2              True          True  \n3              True          True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>study_id</th>\n      <th>condition</th>\n      <th>level</th>\n      <th>severity</th>\n      <th>series_id</th>\n      <th>instance_number</th>\n      <th>x</th>\n      <th>y</th>\n      <th>series_description</th>\n      <th>image_path</th>\n      <th>study_id_exists</th>\n      <th>series_id_exists</th>\n      <th>image_exists</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L1/L2</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>322.831858</td>\n      <td>227.964602</td>\n      <td>Sagittal T2/STIR</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L2/L3</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>320.571429</td>\n      <td>295.714286</td>\n      <td>Sagittal T2/STIR</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L3/L4</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>323.030303</td>\n      <td>371.818182</td>\n      <td>Sagittal T2/STIR</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4003253</td>\n      <td>Spinal Canal Stenosis</td>\n      <td>L4/L5</td>\n      <td>0.0</td>\n      <td>702807833</td>\n      <td>8</td>\n      <td>335.292035</td>\n      <td>427.327434</td>\n      <td>Sagittal T2/STIR</td>\n      <td>/kaggle/input/rsna-2024-lumbar-spine-degenerat...</td>\n      <td>True</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Dictionary to store grouped data\ngrouped_data = {}\n\n# Loop through each row and build the dictionary for each unique (study_id, series_id, instance_number) combination\nfor _, row in train.iterrows():\n    key = (row['study_id'], row['series_id'], row['instance_number'])\n    \n    # Initialize a new entry in the grouped_data dictionary if this key is not yet present\n    if key not in grouped_data:\n        grouped_data[key] = {\n            'study_id': row['study_id'],\n            'series_id': row['series_id'],\n            'instance_number': row['instance_number'],\n            'condition':row['condition'],\n            'conditions': {},\n            'series_description':row['series_description'],\n            'image_path':row['image_path']\n        }\n    \n    # Add the level-severity pair to the conditions dictionary for this key\n    grouped_data[key]['conditions'][row['level']] = row['severity']\n\n# Convert the grouped data dictionary to a DataFrame\ntrain_transformed = pd.DataFrame(grouped_data.values())\n\n# Display the transformed DataFrame\ntrain_transformed.head(2)\ntrain = train_transformed","metadata":{"papermill":{"duration":4.289857,"end_time":"2024-10-28T14:19:20.741986","exception":false,"start_time":"2024-10-28T14:19:16.452129","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:08:38.895275Z","iopub.execute_input":"2024-10-29T02:08:38.895598Z","iopub.status.idle":"2024-10-29T02:08:43.078724Z","shell.execute_reply.started":"2024-10-29T02:08:38.895565Z","shell.execute_reply":"2024-10-29T02:08:43.077729Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)","metadata":{"papermill":{"duration":0.088754,"end_time":"2024-10-28T14:19:20.838302","exception":false,"start_time":"2024-10-28T14:19:20.749548","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:08:43.080019Z","iopub.execute_input":"2024-10-29T02:08:43.080401Z","iopub.status.idle":"2024-10-29T02:08:43.086288Z","shell.execute_reply.started":"2024-10-29T02:08:43.080359Z","shell.execute_reply":"2024-10-29T02:08:43.085327Z"},"trusted":true},"execution_count":35,"outputs":[{"name":"stdout","text":"cuda\n","output_type":"stream"}]},{"cell_type":"code","source":"class MRIData(Dataset):\n    def __init__(self, dataframe, transform=None):\n        self.dataframe = dataframe\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.dataframe)\n\n    def __getitem__(self, index):\n        image_path = self.dataframe['image_path'][index]\n        image = load_dicom(image_path)  # Define this function to load your DICOM images\n        \n        # Convert the dictionary of conditions to a numpy vector\n        conditions_dict = self.dataframe['conditions'][index]\n        conditions_vector = self.conditions_to_vector(conditions_dict)\n        \n        # Optionally transform the image\n        if self.transform:\n            image = self.transform(image)\n\n        return image, conditions_vector\n\n    def conditions_to_vector(self, conditions_dict):\n        levels = ['L1/L2', 'L2/L3', 'L3/L4', 'L4/L5', 'L5/S1']  # Ajusta seg√∫n tus niveles\n        vector = np.array([conditions_dict.get(level, 0) for level in levels])\n        return vector","metadata":{"papermill":{"duration":0.017696,"end_time":"2024-10-28T14:19:20.863539","exception":false,"start_time":"2024-10-28T14:19:20.845843","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:08:43.091079Z","iopub.execute_input":"2024-10-29T02:08:43.091413Z","iopub.status.idle":"2024-10-29T02:08:43.100082Z","shell.execute_reply.started":"2024-10-29T02:08:43.091380Z","shell.execute_reply":"2024-10-29T02:08:43.099262Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Function to create datasets and dataloaders for each series description\ndef create_datasets_and_loaders(df, series_description, transform, batch_size=8):\n    filtered_df = df[df['series_description'] == series_description]\n    \n    train_df, val_df = train_test_split(filtered_df, test_size=0.2, random_state=42)\n    train_df = train_df.reset_index(drop=True)\n    val_df = val_df.reset_index(drop=True)\n\n    train_dataset = MRIData(train_df, transform)\n    val_dataset = MRIData(val_df, transform)\n\n    trainloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n    \n    return trainloader, valloader, len(train_df), len(val_df)","metadata":{"papermill":{"duration":0.015859,"end_time":"2024-10-28T14:19:20.886581","exception":false,"start_time":"2024-10-28T14:19:20.870722","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:08:43.101268Z","iopub.execute_input":"2024-10-29T02:08:43.101641Z","iopub.status.idle":"2024-10-29T02:08:43.113345Z","shell.execute_reply.started":"2024-10-29T02:08:43.101598Z","shell.execute_reply":"2024-10-29T02:08:43.112581Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"def load_dicom(path):\n    dc = pydicom.dcmread(path)\n    data = dc.pixel_array\n    data = data - np.min(data)\n    if np.max(data) != 0:\n        data = data / np.max(data)\n    data = (data * 255).astype(np.uint8)\n    return data","metadata":{"papermill":{"duration":0.014877,"end_time":"2024-10-28T14:19:20.908592","exception":false,"start_time":"2024-10-28T14:19:20.893715","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:08:43.114468Z","iopub.execute_input":"2024-10-29T02:08:43.114818Z","iopub.status.idle":"2024-10-29T02:08:43.124229Z","shell.execute_reply.started":"2024-10-29T02:08:43.114786Z","shell.execute_reply":"2024-10-29T02:08:43.123408Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Define the transforms\ntransform = transforms.Compose([\n    transforms.Lambda(lambda x: (x * 255).astype(np.uint8)),  # Convert back to uint8 for PIL\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.Grayscale(num_output_channels=3),\n    transforms.ToTensor(),\n])","metadata":{"papermill":{"duration":0.014897,"end_time":"2024-10-28T14:19:20.930580","exception":false,"start_time":"2024-10-28T14:19:20.915683","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:08:43.125421Z","iopub.execute_input":"2024-10-29T02:08:43.125716Z","iopub.status.idle":"2024-10-29T02:08:43.135770Z","shell.execute_reply.started":"2024-10-29T02:08:43.125686Z","shell.execute_reply":"2024-10-29T02:08:43.135009Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"criterion = nn.CrossEntropyLoss()","metadata":{"papermill":{"duration":0.014491,"end_time":"2024-10-28T14:19:20.952204","exception":false,"start_time":"2024-10-28T14:19:20.937713","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:08:43.136737Z","iopub.execute_input":"2024-10-29T02:08:43.137059Z","iopub.status.idle":"2024-10-29T02:08:43.149230Z","shell.execute_reply.started":"2024-10-29T02:08:43.137020Z","shell.execute_reply":"2024-10-29T02:08:43.148507Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"class SpinalConditionCNN(nn.Module):\n    def __init__(self, num_levels=5):  # Assuming 3 classes per level (e.g., normal, mild, severe)\n        super(SpinalConditionCNN, self).__init__()\n        self.conv_layers = nn.Sequential(\n            nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2)\n        )\n        self.fc_layers = nn.Sequential(\n            nn.Flatten(),\n            nn.Linear(128 * 28 * 28, 512),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(512, num_levels)\n        )\n        \n        self.num_levels = num_levels\n\n\n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = self.fc_layers(x)\n        x = x.view(-1, self.num_levels)  # Shape (batch_size, num_levels)\n        return x","metadata":{"papermill":{"duration":0.01836,"end_time":"2024-10-28T14:19:20.977912","exception":false,"start_time":"2024-10-28T14:19:20.959552","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:08:43.150409Z","iopub.execute_input":"2024-10-29T02:08:43.150762Z","iopub.status.idle":"2024-10-29T02:08:43.160842Z","shell.execute_reply.started":"2024-10-29T02:08:43.150726Z","shell.execute_reply":"2024-10-29T02:08:43.159940Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def train_model(model, trainloader, valloader, criterion, optimizer, num_epochs=200, device='cuda'):\n    model = model.to(device)\n    best_val_loss = float('inf')\n    train_losses = []\n    val_losses = []\n    train_accuracies = []\n    val_accuracies = []\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct_train = 0\n        total_train = 0\n        for images, labels in trainloader:\n            images, labels = images.to(device), labels.to(device)\n            \n            # Forward pass\n            outputs = model(images)\n            loss = criterion(outputs.view(-1, model.num_levels), labels)\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            predicted,_  = torch.max(outputs, 1)\n            running_loss += loss.item() * images.size(0)\n            correct_train += (predicted == labels).sum().item()\n            total_train += labels.size(0)\n\n        # Calculate average loss for this epoch\n        epoch_loss = running_loss / len(trainloader.dataset)\n        train_losses.append(epoch_loss)\n        train_accuracy = correct_train / total_train\n        train_accuracies.append(train_accuracy)\n        \n        # Validation phase\n        model.eval()\n        val_loss = 0.0\n        correct_val = 0\n        total_val = 0\n        with torch.no_grad():\n            for images, labels in valloader:\n                images, labels = images.to(device), labels.to(device)\n                outputs = model(images)\n                loss = criterion(outputs.view(-1, model.num_levels), labels)\n                val_loss += loss.item() * images.size(0)\n                _, predicted = torch.max(outputs, 1)\n                correct_val += (predicted == labels).sum().item()\n                total_val += labels.size(0)\n\n        val_loss /= len(valloader.dataset)\n        val_losses.append(val_loss)\n        val_accuracy = correct_val / total_val\n        val_accuracies.append(val_accuracy)\n        \n        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Train Acc: {train_accuracy:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_accuracy:.4f}')\n        \n    # Plotting the loss and accuracy curves\n    epochs = range(1, num_epochs + 1)\n    plt.figure(figsize=(14, 5))\n\n    # Plot Loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, 'bo-', label='Training Loss')\n    plt.plot(epochs, val_losses, 'ro-', label='Validation Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss')\n    plt.legend()\n\n    # Plot Accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_accuracies, 'bo-', label='Training Accuracy')\n    plt.plot(epochs, val_accuracies, 'ro-', label='Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy')\n    plt.title('Training and Validation Accuracy')\n    plt.legend()\n\n    plt.tight_layout()\n    plt.show()\n\n    print('Training complete')\n    return model\n","metadata":{"papermill":{"duration":0.025314,"end_time":"2024-10-28T14:19:21.010289","exception":false,"start_time":"2024-10-28T14:19:20.984975","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:08:43.162322Z","iopub.execute_input":"2024-10-29T02:08:43.162618Z","iopub.status.idle":"2024-10-29T02:08:43.179579Z","shell.execute_reply.started":"2024-10-29T02:08:43.162587Z","shell.execute_reply":"2024-10-29T02:08:43.178555Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Assuming df is your dataframe with transformed conditions\ntrainloader, valloader, _, _ = create_datasets_and_loaders(train, series_description='Sagittal T2/STIR', transform=transform, batch_size=5)\n# Initialize model, loss, and optimizer\nmodel = SpinalConditionCNN(num_levels=5)  # Adjust num_classes based on the severity levels in your data\ncriterion = nn.CrossEntropyLoss()  # Assuming conditions are categorical with class labels for severity\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# Train the model\ntrained_model = train_model(model, trainloader, valloader, criterion, optimizer, num_epochs=200, device='cuda' if torch.cuda.is_available() else 'cpu')\ntorch.save(trained_model.state_dict(),'Sagittal_T2_STIR.pth')","metadata":{"papermill":{"duration":5587.688241,"end_time":"2024-10-28T15:52:28.705631","exception":false,"start_time":"2024-10-28T14:19:21.017390","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-10-29T02:08:43.180654Z","iopub.execute_input":"2024-10-29T02:08:43.180929Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch [1/200], Train Loss: 0.9699, Train Acc: 0.0000, Val Loss: 0.7941, Val Acc: 0.0000\nEpoch [2/200], Train Loss: 0.9276, Train Acc: 0.0000, Val Loss: 0.8025, Val Acc: 0.0000\nEpoch [3/200], Train Loss: 0.9225, Train Acc: 0.0000, Val Loss: 0.7914, Val Acc: 0.0000\nEpoch [4/200], Train Loss: 0.9392, Train Acc: 0.0000, Val Loss: 0.7929, Val Acc: 0.0000\nEpoch [5/200], Train Loss: 0.9239, Train Acc: 0.0000, Val Loss: 0.7912, Val Acc: 0.0000\nEpoch [6/200], Train Loss: 0.9176, Train Acc: 0.0000, Val Loss: 0.7947, Val Acc: 0.0000\nEpoch [7/200], Train Loss: 0.9331, Train Acc: 0.0000, Val Loss: 0.7918, Val Acc: 0.0000\nEpoch [8/200], Train Loss: 0.9176, Train Acc: 0.0000, Val Loss: 0.7935, Val Acc: 0.0000\nEpoch [9/200], Train Loss: 0.9193, Train Acc: 0.0000, Val Loss: 0.7887, Val Acc: 0.0000\nEpoch [10/200], Train Loss: 0.9130, Train Acc: 0.0000, Val Loss: 0.7946, Val Acc: 0.0000\nEpoch [11/200], Train Loss: 0.9110, Train Acc: 0.0000, Val Loss: 0.7906, Val Acc: 0.0000\nEpoch [12/200], Train Loss: 0.9118, Train Acc: 0.0000, Val Loss: 0.7898, Val Acc: 0.0000\nEpoch [13/200], Train Loss: 0.9118, Train Acc: 0.0000, Val Loss: 0.7917, Val Acc: 0.0000\nEpoch [14/200], Train Loss: 0.9186, Train Acc: 0.0000, Val Loss: 0.7901, Val Acc: 0.0000\nEpoch [15/200], Train Loss: 0.9142, Train Acc: 0.0000, Val Loss: 0.7899, Val Acc: 0.0000\nEpoch [16/200], Train Loss: 0.9116, Train Acc: 0.0000, Val Loss: 0.7864, Val Acc: 0.0000\nEpoch [17/200], Train Loss: 0.9097, Train Acc: 0.0000, Val Loss: 0.7857, Val Acc: 0.0020\nEpoch [18/200], Train Loss: 0.9056, Train Acc: 0.0000, Val Loss: 0.7864, Val Acc: 0.0020\nEpoch [19/200], Train Loss: 0.9031, Train Acc: 0.0000, Val Loss: 0.7856, Val Acc: 0.0020\nEpoch [20/200], Train Loss: 0.8991, Train Acc: 0.0000, Val Loss: 0.7874, Val Acc: 0.0059\nEpoch [21/200], Train Loss: 0.8947, Train Acc: 0.0000, Val Loss: 0.7899, Val Acc: 0.0059\nEpoch [22/200], Train Loss: 0.8754, Train Acc: 0.0000, Val Loss: 0.7868, Val Acc: 0.0218\nEpoch [23/200], Train Loss: 0.8459, Train Acc: 0.0000, Val Loss: 0.7878, Val Acc: 0.0238\nEpoch [24/200], Train Loss: 0.8039, Train Acc: 0.0000, Val Loss: 0.7957, Val Acc: 0.0475\nEpoch [25/200], Train Loss: 0.7484, Train Acc: 0.0000, Val Loss: 0.8820, Val Acc: 0.0792\nEpoch [26/200], Train Loss: 0.7036, Train Acc: 0.0000, Val Loss: 0.9815, Val Acc: 0.1228\nEpoch [27/200], Train Loss: 0.6726, Train Acc: 0.0000, Val Loss: 0.9290, Val Acc: 0.0733\nEpoch [28/200], Train Loss: 0.6267, Train Acc: 0.0000, Val Loss: 1.0491, Val Acc: 0.1089\nEpoch [29/200], Train Loss: 0.6059, Train Acc: 0.0000, Val Loss: 1.0095, Val Acc: 0.1208\nEpoch [30/200], Train Loss: 0.5968, Train Acc: 0.0000, Val Loss: 1.0535, Val Acc: 0.0772\nEpoch [31/200], Train Loss: 0.5812, Train Acc: 0.0000, Val Loss: 0.9915, Val Acc: 0.1248\nEpoch [32/200], Train Loss: 0.5650, Train Acc: 0.0000, Val Loss: 1.0593, Val Acc: 0.1208\nEpoch [33/200], Train Loss: 0.5470, Train Acc: 0.0000, Val Loss: 1.0537, Val Acc: 0.1683\nEpoch [34/200], Train Loss: 0.5350, Train Acc: 0.0000, Val Loss: 1.0700, Val Acc: 0.1228\nEpoch [35/200], Train Loss: 0.5347, Train Acc: 0.0000, Val Loss: 1.1488, Val Acc: 0.1010\nEpoch [36/200], Train Loss: 0.5172, Train Acc: 0.0000, Val Loss: 1.1642, Val Acc: 0.1267\nEpoch [37/200], Train Loss: 0.5029, Train Acc: 0.0000, Val Loss: 1.0782, Val Acc: 0.0990\nEpoch [38/200], Train Loss: 0.5040, Train Acc: 0.0000, Val Loss: 1.0656, Val Acc: 0.0911\nEpoch [39/200], Train Loss: 0.4908, Train Acc: 0.0000, Val Loss: 1.2407, Val Acc: 0.1545\nEpoch [40/200], Train Loss: 0.4868, Train Acc: 0.0000, Val Loss: 1.1373, Val Acc: 0.1149\nEpoch [41/200], Train Loss: 0.4926, Train Acc: 0.0000, Val Loss: 1.1256, Val Acc: 0.1703\nEpoch [42/200], Train Loss: 0.4789, Train Acc: 0.0000, Val Loss: 1.1695, Val Acc: 0.1248\nEpoch [43/200], Train Loss: 0.4798, Train Acc: 0.0000, Val Loss: 1.1712, Val Acc: 0.0891\nEpoch [44/200], Train Loss: 0.4673, Train Acc: 0.0000, Val Loss: 1.1002, Val Acc: 0.1505\nEpoch [45/200], Train Loss: 0.4656, Train Acc: 0.0000, Val Loss: 1.2183, Val Acc: 0.1386\nEpoch [46/200], Train Loss: 0.4626, Train Acc: 0.0000, Val Loss: 1.2086, Val Acc: 0.1089\nEpoch [47/200], Train Loss: 0.4526, Train Acc: 0.0000, Val Loss: 1.2348, Val Acc: 0.0931\nEpoch [48/200], Train Loss: 0.4492, Train Acc: 0.0000, Val Loss: 1.2199, Val Acc: 0.1149\nEpoch [49/200], Train Loss: 0.4523, Train Acc: 0.0000, Val Loss: 1.2393, Val Acc: 0.0931\nEpoch [50/200], Train Loss: 0.4381, Train Acc: 0.0000, Val Loss: 1.1470, Val Acc: 0.1050\nEpoch [51/200], Train Loss: 0.4458, Train Acc: 0.0000, Val Loss: 1.3131, Val Acc: 0.1327\nEpoch [52/200], Train Loss: 0.4400, Train Acc: 0.0000, Val Loss: 1.4033, Val Acc: 0.1069\nEpoch [53/200], Train Loss: 0.4393, Train Acc: 0.0000, Val Loss: 1.3994, Val Acc: 0.1109\nEpoch [54/200], Train Loss: 0.4414, Train Acc: 0.0000, Val Loss: 1.3901, Val Acc: 0.1050\nEpoch [55/200], Train Loss: 0.4348, Train Acc: 0.0000, Val Loss: 1.3988, Val Acc: 0.0871\nEpoch [56/200], Train Loss: 0.4341, Train Acc: 0.0000, Val Loss: 1.3767, Val Acc: 0.1168\nEpoch [57/200], Train Loss: 0.4251, Train Acc: 0.0000, Val Loss: 1.2972, Val Acc: 0.1426\nEpoch [58/200], Train Loss: 0.4314, Train Acc: 0.0000, Val Loss: 1.3841, Val Acc: 0.1327\nEpoch [59/200], Train Loss: 0.4264, Train Acc: 0.0000, Val Loss: 1.3917, Val Acc: 0.1386\nEpoch [60/200], Train Loss: 0.4213, Train Acc: 0.0000, Val Loss: 1.4025, Val Acc: 0.1050\nEpoch [61/200], Train Loss: 0.4161, Train Acc: 0.0000, Val Loss: 1.2975, Val Acc: 0.1287\nEpoch [62/200], Train Loss: 0.4108, Train Acc: 0.0000, Val Loss: 1.3637, Val Acc: 0.1347\nEpoch [63/200], Train Loss: 0.4080, Train Acc: 0.0000, Val Loss: 1.4559, Val Acc: 0.1287\nEpoch [64/200], Train Loss: 0.4098, Train Acc: 0.0000, Val Loss: 1.3630, Val Acc: 0.1069\nEpoch [65/200], Train Loss: 0.4081, Train Acc: 0.0000, Val Loss: 1.4333, Val Acc: 0.0812\nEpoch [66/200], Train Loss: 0.4071, Train Acc: 0.0000, Val Loss: 1.4503, Val Acc: 0.1327\nEpoch [67/200], Train Loss: 0.4053, Train Acc: 0.0000, Val Loss: 1.5156, Val Acc: 0.1188\nEpoch [68/200], Train Loss: 0.4015, Train Acc: 0.0000, Val Loss: 1.6161, Val Acc: 0.1426\nEpoch [69/200], Train Loss: 0.3987, Train Acc: 0.0000, Val Loss: 1.5382, Val Acc: 0.1446\nEpoch [70/200], Train Loss: 0.4008, Train Acc: 0.0000, Val Loss: 1.5912, Val Acc: 0.1069\nEpoch [71/200], Train Loss: 0.3968, Train Acc: 0.0000, Val Loss: 1.6517, Val Acc: 0.1307\nEpoch [72/200], Train Loss: 0.3957, Train Acc: 0.0000, Val Loss: 1.3069, Val Acc: 0.1129\nEpoch [73/200], Train Loss: 0.3917, Train Acc: 0.0000, Val Loss: 1.6809, Val Acc: 0.1129\nEpoch [74/200], Train Loss: 0.3942, Train Acc: 0.0000, Val Loss: 1.6047, Val Acc: 0.1347\nEpoch [75/200], Train Loss: 0.3943, Train Acc: 0.0000, Val Loss: 1.6366, Val Acc: 0.1921\nEpoch [76/200], Train Loss: 0.3891, Train Acc: 0.0000, Val Loss: 1.6095, Val Acc: 0.0891\nEpoch [77/200], Train Loss: 0.3975, Train Acc: 0.0000, Val Loss: 1.7912, Val Acc: 0.1545\nEpoch [78/200], Train Loss: 0.3988, Train Acc: 0.0000, Val Loss: 1.7748, Val Acc: 0.1149\nEpoch [79/200], Train Loss: 0.3889, Train Acc: 0.0000, Val Loss: 1.8056, Val Acc: 0.1287\nEpoch [80/200], Train Loss: 0.3846, Train Acc: 0.0000, Val Loss: 1.6058, Val Acc: 0.1644\nEpoch [81/200], Train Loss: 0.3843, Train Acc: 0.0000, Val Loss: 1.7937, Val Acc: 0.0970\nEpoch [82/200], Train Loss: 0.3932, Train Acc: 0.0000, Val Loss: 1.7019, Val Acc: 0.0911\nEpoch [83/200], Train Loss: 0.3828, Train Acc: 0.0000, Val Loss: 1.9012, Val Acc: 0.1089\nEpoch [84/200], Train Loss: 0.3813, Train Acc: 0.0000, Val Loss: 1.9454, Val Acc: 0.1564\nEpoch [85/200], Train Loss: 0.3882, Train Acc: 0.0000, Val Loss: 2.0163, Val Acc: 0.1327\nEpoch [86/200], Train Loss: 0.3798, Train Acc: 0.0000, Val Loss: 1.8250, Val Acc: 0.1149\nEpoch [87/200], Train Loss: 0.3847, Train Acc: 0.0000, Val Loss: 1.8452, Val Acc: 0.1030\nEpoch [88/200], Train Loss: 0.3851, Train Acc: 0.0000, Val Loss: 1.3916, Val Acc: 0.1644\nEpoch [89/200], Train Loss: 0.3880, Train Acc: 0.0000, Val Loss: 1.9884, Val Acc: 0.0792\nEpoch [90/200], Train Loss: 0.3765, Train Acc: 0.0000, Val Loss: 1.9571, Val Acc: 0.1228\nEpoch [91/200], Train Loss: 0.3875, Train Acc: 0.0000, Val Loss: 2.1197, Val Acc: 0.1584\nEpoch [92/200], Train Loss: 0.3796, Train Acc: 0.0000, Val Loss: 1.7601, Val Acc: 0.1228\nEpoch [93/200], Train Loss: 0.3827, Train Acc: 0.0000, Val Loss: 1.8691, Val Acc: 0.1069\nEpoch [94/200], Train Loss: 0.3790, Train Acc: 0.0000, Val Loss: 1.8140, Val Acc: 0.1307\nEpoch [95/200], Train Loss: 0.3759, Train Acc: 0.0000, Val Loss: 1.8114, Val Acc: 0.1446\nEpoch [96/200], Train Loss: 0.3806, Train Acc: 0.0000, Val Loss: 1.7639, Val Acc: 0.1267\nEpoch [97/200], Train Loss: 0.3745, Train Acc: 0.0000, Val Loss: 1.7757, Val Acc: 0.1208\nEpoch [98/200], Train Loss: 0.3765, Train Acc: 0.0000, Val Loss: 2.1436, Val Acc: 0.1762\nEpoch [99/200], Train Loss: 0.3752, Train Acc: 0.0000, Val Loss: 2.0146, Val Acc: 0.1485\nEpoch [100/200], Train Loss: 0.3785, Train Acc: 0.0000, Val Loss: 1.9987, Val Acc: 0.1941\nEpoch [101/200], Train Loss: 0.3754, Train Acc: 0.0000, Val Loss: 1.9239, Val Acc: 0.1188\nEpoch [102/200], Train Loss: 0.3741, Train Acc: 0.0000, Val Loss: 2.3109, Val Acc: 0.1386\nEpoch [103/200], Train Loss: 0.3825, Train Acc: 0.0000, Val Loss: 2.3368, Val Acc: 0.1228\nEpoch [104/200], Train Loss: 0.3775, Train Acc: 0.0000, Val Loss: 2.2889, Val Acc: 0.1584\nEpoch [105/200], Train Loss: 0.3752, Train Acc: 0.0000, Val Loss: 2.3517, Val Acc: 0.2178\nEpoch [106/200], Train Loss: 0.3758, Train Acc: 0.0000, Val Loss: 2.0442, Val Acc: 0.2000\nEpoch [107/200], Train Loss: 0.3727, Train Acc: 0.0000, Val Loss: 2.0553, Val Acc: 0.1921\nEpoch [108/200], Train Loss: 0.3735, Train Acc: 0.0000, Val Loss: 1.9885, Val Acc: 0.1307\nEpoch [109/200], Train Loss: 0.3746, Train Acc: 0.0000, Val Loss: 2.0803, Val Acc: 0.1564\nEpoch [110/200], Train Loss: 0.3814, Train Acc: 0.0000, Val Loss: 1.9088, Val Acc: 0.1525\nEpoch [111/200], Train Loss: 0.3726, Train Acc: 0.0000, Val Loss: 2.0809, Val Acc: 0.1723\nEpoch [112/200], Train Loss: 0.3741, Train Acc: 0.0000, Val Loss: 2.2740, Val Acc: 0.1842\nEpoch [113/200], Train Loss: 0.3750, Train Acc: 0.0000, Val Loss: 2.1882, Val Acc: 0.1307\nEpoch [114/200], Train Loss: 0.3777, Train Acc: 0.0000, Val Loss: 1.9814, Val Acc: 0.1267\nEpoch [115/200], Train Loss: 0.3765, Train Acc: 0.0000, Val Loss: 2.0433, Val Acc: 0.1267\nEpoch [116/200], Train Loss: 0.3793, Train Acc: 0.0000, Val Loss: 1.8187, Val Acc: 0.1762\nEpoch [117/200], Train Loss: 0.3715, Train Acc: 0.0000, Val Loss: 2.0257, Val Acc: 0.1287\nEpoch [118/200], Train Loss: 0.3711, Train Acc: 0.0000, Val Loss: 2.0008, Val Acc: 0.1683\nEpoch [119/200], Train Loss: 0.3722, Train Acc: 0.0000, Val Loss: 2.2293, Val Acc: 0.1584\nEpoch [120/200], Train Loss: 0.3738, Train Acc: 0.0000, Val Loss: 2.2709, Val Acc: 0.1624\n","output_type":"stream"}]},{"cell_type":"code","source":"# Assuming df is your dataframe with transformed conditions\ntrainloader_ax, valloader_ax, _, _ = create_datasets_and_loaders(train, series_description='Axial T2', transform=transform, batch_size=5)\n# Initialize model, loss, and optimizer\nmodel = SpinalConditionCNN(num_levels=5)  # Adjust num_classes based on the severity levels in your data\ncriterion = nn.CrossEntropyLoss()  # Assuming conditions are categorical with class labels for severity\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# Train the model\ntrained_model = train_model(model, trainloader_ax, valloader_ax, criterion, optimizer, num_epochs=200, device='cuda' if torch.cuda.is_available() else 'cpu')\ntorch.save(trained_model.state_dict(),'Axial_T2.pth')","metadata":{"papermill":{"duration":334.247209,"end_time":"2024-10-28T15:58:02.969085","exception":true,"start_time":"2024-10-28T15:52:28.721876","status":"failed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assuming df is your dataframe with transformed conditions\ntrainloader_t1, valloader_t1, _, _ = create_datasets_and_loaders(train, series_description='Sagittal T1', transform=transform, batch_size=5)\n# Initialize model, loss, and optimizer\nmodel = SpinalConditionCNN(num_levels=5)  # Adjust num_classes based on the severity levels in your data\ncriterion = nn.CrossEntropyLoss()  # Assuming conditions are categorical with class labels for severity\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n# Train the model\ntrained_model = train_model(model, trainloader_t1, valloader_t1, criterion, optimizer, num_epochs=200, device='cuda' if torch.cuda.is_available() else 'cpu')\ntorch.save(trained_model.state_dict(),'Sagittal_T1.pth')","metadata":{"papermill":{"duration":null,"end_time":null,"exception":null,"start_time":null,"status":"pending"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}